---
title: "How Consumers Evaluate AI Transparency"
date: 2018-10-02T15:43:56+02:00
url: /posts/ai-transparency/
draft: True
---
<!--
TODO:
- update date
-->
This post is the by-product of a study that a colleague and I conducted this
spring, examining whether and why consumers are willing to pay for transparency
features in intelligent systems. We were able to collect some additional data
on consumers' evaluation of  transparency in the context of five AI applications.
My aim for this post is to share results from this auxiliary study with a broader
audience in a semi-scientific way. Let's start by motivating the topic...

## Why is AI transparency important?
<!--
Points to cover:
- Include main points from paper introduction
-->
First of all, we have to establish some terminology. Generally, I agree with
McCarthy's (2007) definition of AI:

> [Artificial intelligence] is the science is the science and engineering of making intelligent machines, especially intelligent computer programs.

Even though I often use the terms _AI_ and _intelligent systems_ interchangeably,
I regard the latter to constitute the artifact that users are interacting with.
Moreover, I assume the system to comprise the following components:

- Input and output data
- Backend including business logic
- User interface
- Algorithm details, including statistical model and training/validation data

## How did we examine user evaluation?
<!--
Points to cover:
- Constructs and questions
- Use cases
- Sample
-->

## What did we find?
<!--
Points to cover:
- Mean differences in evaluation metrics between use cases
- What influences usefulness/intention the most?
-->

## What can we learn from this?
